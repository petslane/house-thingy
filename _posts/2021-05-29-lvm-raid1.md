---
layout: post
title: Setting up LVM raid 1 and backing up SD card
excerpt_separator: <!--more-->
---

Now as Raspberry Pi is running from USB attached SSD, it's time to add a second SSD and set up LVM Raid1 for increased resilience. Backup SD card. And some performance tests with different raid 1 configurations.

<!--more-->

## USB SSDs

I got 2 brand new ADATA SU630 240GB SSD-s attached with a USB3-SATA adapter. For both adapters, quirks are enabled. One drive is empty and another contains the current system on 3 LVs - root, docker, and swap. 
I am using 2 different USB3-SATA adapters. One is `152d:0578` that I got from Aliexpress where item description explicitly said to support `UASP`, but when I got it and saw device id, then I knew that I already have this one and it does not support `UASP` in Linux. The second one (Axagon ADSA-FP2) I bought from a local store that should support `UASP`, but the device id for that was `152d:0567` and that too does not support `UASP` under Linux. So currently both SSD-s are using a `usb-storage` driver, which is not the best solution but satisfies my current need. If I should somehow get new adapters that support `UASP`, then I will definitely do new performance tests.

## Creating Raid 1

To convert existing linear LV into raid 1 you need your VG to contain multiple PV-s. Assuming new SSD is attached as `/dev/sdb`, then for creating PV you need to execute command `pvcreate /dev/sdb`. Then you need to extend the existing VG to contain that drive with the command `vgextend pivg /dev/sdb`. Now you can create raid1 with 1 mirror from any LV within extended VG with command `lvconvert --type raid1 --mirrors 1 pivg/piroot`. You can have also 3rd drive, create PV on it and extend VG to that too, and using `--mirrors 2` you can have raid 1 with 3 copies allowing you to keep your system online with 2 failed drives.

## Performance testing

SSD performance tests were made with [pibenchmarks](https://pibenchmarks.com/). The first 3 tests (HDParm, DD) are testing reading/writing of big files. The other 6 tests (FIO, IOZone) are testing reading/writing of small files - these are actually more important to system overall performance. Because both disks are using `usb-storage` driver, then FIO and IOZone 4k reading/writing results are probably significantly slower because of this. But it is still possible to compare different disk configurations.

Configurations:
- Linear USB 2 - this is 1 SSD connected to USB2. No raid. It's just to compare with USB3.
- Linear USB 3 - this is 1 SSD connected to USB3. No raid. This is probably the most common Pi 4 setup that uses an external SSD.
- Raid1 USB2-USB2 - this is 2 SSDs connected to USB2 ports. Raid 1. It's for comparing with linear USB2.
- Raid1 USB2-USB3 - same as the previous configuration, but one disk is connected to a faster USB3 port.
- Raid1 USB2-USB3 (writemostly) - same as the previous configuration, but the slower disk is configured with `--writemostly` attribute. It's for comparing performance where one of the disks is connected to a slow USB port and raid is configured with "writemostly" for USB2 disk. With this configuration Raid1 will write to all disks, but reading is done from USB3 disk only, so reading should be fast, but writing should be still limited to the USB2 speed.
- Raid1 USB2-USB3 (writemostly, writebehind) - same as the previous configuration, but the slower disk is additionally configured with `--writebehind 30000` attribute. With this configuration, Raid1 will allow write lag for the slower disk that should make writing faster.
- Raid1 USB3-USB3 - this is 2 SSDs connected to USB3 ports. Raid 1. This is desired configuration.

All tests were run 3 times and the chart shows an average of 3 runs.

<iframe width="1048" height="648" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRO4Q1D_6lhOR098SbnJQeVPPUkovhhwQOjDYjNJaDUQFPOSZD6Cej1XfZMbNFEu9lIl38iyEkCZfci/pubhtml?gid=1379708017&amp;single=true&amp;widget=true&amp;headers=false"></iframe>

With no-raid configuration, USB2 is slowest and USB3 is fastest where speeds are limited by USB port speed (additional limitation for USB3 by using `usb-storage` driver). By configuring Raid 1 and using same type of USB ports will usually keep read speeds, but write speeds get reduced.

But what about Raid 1 that is configured on different USB speeds - one disk on USB3 and another on USB2? With default raid settings, reading/writing will be limited to the slowest port, but it will be faster than Raid 1 on USB2 ports, it's about as fast as Linear USB2.

For Raid 1 configuration, it's possible to mark slow disks with `writemostly` attribute. With this attribute, reading is done from fast disks only. And tests show that reading is done with USB3 speeds and writing is done with USB2 speeds.

Then there is an additional configuration option - `writebehind`. With this configuration, all writes will be done to all disks, but it will not wait for write confirmation from slow disks. This should make writes faster and allow slower disks to sync up later. But with my test, writing was still limited by USB2. So not sure why this did not work.

So, if you need to use Raid 1 with USB3 and USB2, or maybe with SSD and HDD, then it's ok, but you have to set `writemostly` and probably `writebehind`.

I would really like to do same tests with USB3 ports with USB3-SATA adapter that supports `uas` driver.

I also tried to do tests by connecting 2 disks to USB3 hub, but this did not work. The system did not boot up, probably there was not enough power from one USB3 port to power hub and 2 disks. This hub has an external power possibility, but I do not have a power adapter for it. If I get it powered, I will do some testing. Theoretically, speeds should be the same as using two USB3 ports as 5Gbps port speed should be enough for both disks, but you never know unless you test it.

# Refresh needed

With my LVM raid 1 setup on Raspberry Pi 4, every time I restarted Pi, it started up, but for some reason LV required refreshing. This usually happens when you disconnect one disk and then connect it back. Then status of the reattached disk will be "Refresh needed". To fix that, you have to execute a command for every LV that requires refresh - `lvchange --refresh LG/LV`. This will synchronize LV-s. I'm not completely sure why this happens, I suspect that one of the SATA-USB3 adapters takes time to startup, and at the moment when Pi starts, one of the disks is not usable, then raid will be marked as "refresh needed" - one disk missing. I suspect that if I somehow managed to delay system startup by couple of seconds (maybe there are some settings for `cmdline.txt`?), then I wouldn't have that issue. Or maybe use some other SATA-USB3 adapter.

But because this happened on every reboot, I needed some automatic way to fix it after every boot. For that I created `/etc/init.d/lvm_refresh_raid_lvs` file containing:
```
#! /bin/bash

### BEGIN INIT INFO
# Provides:          lvm_refresh_raid_lvs
# Required-Start:    $local_fs $network
# Required-Stop:     $local_fs
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: lvm_refresh_raid_lvs service
# Description:       lvm_refresh_raid_lvs service
### END INIT INFO

case "$1" in
  start)
    echo "Starting LVM Raid refresh..."

    lvs -o vg_name,lv_name,lv_health_status|grep 'refresh needed'|awk -F ' ' '{printf "lvchange --refresh %s/%s\n", $1, $2}'|bash
    sleep 2
    ;;
  stop)
    ;;
  *)
    exit 1
    ;;
esac

exit 0
```

Make sure to add executable permission to file and make this service start on system startup - `update-rc.d lvm_refresh_raid_lvs defaults`.

Main one-liner that does refresh to LVs is `lvs -o vg_name,lv_name,lv_health_status|grep 'refresh needed'|awk -F ' ' '{printf "lvchange --refresh %s/%s\n", $1, $2}'|bash`. With command `lvs` I request VG name, LV name, and health status that will contain "refresh needed" string in case refresh is needed. Then I grep only rows that have "refresh needed" in health status. With `awk` I split every line by space and use first (VG) and second (LV) column values to construct `lvchange --refresh VG/LV` commands and then pass these commands to `bash` to be executed.

To see your LV statuses I use the command:
```
lvs -a -o+lv_layout,stripes,lv_health_status,raid_sync_action,raid_mismatch_count,devices
```

One very good piece of documentation about the LVM raid is here - [https://www.systutorials.com/docs/linux/man/7-lvmraid/](https://www.systutorials.com/docs/linux/man/7-lvmraid/). This probably contains everything you need, including what to do when one of the disks fails and needs replacing.

# Backing up SD card

Internet is full of examples of how to back up an SD cards with `dd`, so what's the problem?

In my case, I have a 32GB SD card that contains only a `boot` partition with a size of about 256MB. All `dd` examples will make a full image of the SD card, resulting 32GB backup file. Sure, I can compress it down to 256MB or less, but if I would need to restore it, then I have to write all 32GB to a new SD card. This will take a lot of time to write. Of course, I know that only about 256MB of that image is actual data, then I can take let's say 8GB SD card and tell `dd` to write only the first 256MB of the image to the SD card.

But when the time comes and I need to restore backed-up image to the new SD card, I do not want to remember that I need to write only the first 256MB to the SD card and skip the rest. By then I just want to write a backup image to a new SD card without knowing anything about partitions in it. For that, I need to create a backup image with the right size from the beginning. Using `fdisk` it's actually quite simple.

Using the command `fdisk -l /dev/mmcblk0` I get all the information I need for my SD card:
```
Disk /dev/mmcblk0: 29.25 GiB, 31404851200 bytes, 61337600 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xab86aefd

Device         Boot Start    End Sectors  Size Id Type
/dev/mmcblk0p1 *     2048 526335  524288  256M  c W95 FAT32 (LBA)
```

To make a backup image with `dd` containing only actual partitions without any empty space at the end, I need to know the end sector of the last partition. In my case it's `526335` ("End" column of last and actually only partition I have there) and I need to know the sector size - it's 512 in my case ("Sector size").

So very simple `dd` command to backup SD card would be to use `bs=` with value 512 and `count=` with value 526335:
```
time dd bs=512 count=526335 if=/dev/mmcblk0 of=sd.img
526335+0 records in
526335+0 records out
269483520 bytes (269 MB, 257 MiB) copied, 7.45008 s, 36.2 MB/s

real	0m7.460s
user	0m0.799s
sys	0m6.641s
```

The result is `sd.img` file with a size of 257MB. GZip and I got 78MB. I do not expect that my SD card would stop working any time soon as it is used only at boot times and does not get any writes, but if it would stop working for some reason, then all I need to do is find a new SD card and restore the backup image with `dd if=sd.img of=/dev/mmcblk0` command and if writes only 257MB.
